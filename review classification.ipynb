{"cells":[{"cell_type":"markdown","metadata":{"id":"RTBS4sGpph9x"},"source":["Text classification starting from raw text (as\n","a set of text files on disk). We demonstrate the workflow on the IMDB sentiment\n","classification dataset (unprocessed version). We use the `TextVectorization` layer for\n"," word splitting & indexing."]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":1005,"status":"ok","timestamp":1617522567811,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"7RemYQu_ph9y"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"W1mwFE2Nph9z"},"source":["Load the data: Flipkart review classification"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2008,"status":"ok","timestamp":1617522568819,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"MNAFYqOgutgb"},"outputs":[],"source":["dataset = pd.read_csv(\"/content/flip.csv\")"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":2004,"status":"ok","timestamp":1617522568819,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"16mIfrur1vAJ"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","\n","def load_train_test_imdb_data(data_dir):\n","    \"\"\"Loads the IMDB train/test datasets from a folder path.\n","    Input:\n","    data_dir: path to the \"aclImdb\" folder.\n","    \n","    Returns:\n","    train/test datasets as pandas dataframes.\n","    \"\"\"\n","\n","    data = {}\n","    for split in [\"train\", \"test\"]:\n","        data[split] = []\n","        for sentiment in [\"neg\", \"pos\"]:\n","            score = 1 if sentiment == \"pos\" else 0\n","\n","            path = os.path.join(data_dir, split, sentiment)\n","            file_names = os.listdir(path)\n","            for f_name in file_names:\n","                with open(os.path.join(path, f_name), \"r\", encoding=\"utf-8\") as f:\n","                    review = f.read()\n","                    data[split].append([review, score])\n","\n","    np.random.shuffle(data[\"train\"])        \n","    data[\"train\"] = pd.DataFrame(data[\"train\"],\n","                                 columns=['text', 'sentiment'])\n","\n","    np.random.shuffle(data[\"test\"])\n","    data[\"test\"] = pd.DataFrame(data[\"test\"],\n","                                columns=['text', 'sentiment'])\n","\n","    return data[\"train\"], data[\"test\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"elapsed":2625,"status":"error","timestamp":1617522569445,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"AVtXziDhw3tY","outputId":"70319eba-5513-41a1-ac81-a3599cc07fcd"},"outputs":[],"source":["import re\n","\n","\n","def clean_text(text):\n","    \"\"\"\n","    Applies some pre-processing on the given text.\n","\n","    Steps :\n","    - Removing HTML tags\n","    - Removing punctuation\n","    - Lowering text\n","    \"\"\"\n","    \n","    # remove HTML tags\n","    text = re.sub(r'<.*?>', '', text)\n","    \n","    # remove the characters [\\], ['] and [\"]\n","    text = re.sub(r\"\\\\\", \"\", text)    \n","    text = re.sub(r\"\\'\", \"\", text)    \n","    text = re.sub(r\"\\\"\", \"\", text)    \n","    \n","    # convert text to lowercase\n","    text = text.strip().lower()\n","    \n","    # replace punctuation characters with spaces\n","    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n","    translate_dict = dict((c, \" \") for c in filters)\n","    translate_map = str.maketrans(translate_dict)\n","    text = text.translate(translate_map)\n","\n","    return text\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","training_texts = [\n","    \"This is a good cat\",\n","    \"This is a bad day\"\n","]\n","\n","test_texts = [\n","    \"This day is a good day\"\n","]\n","\n","# this vectorizer will skip stop words\n","vectorizer = CountVectorizer(\n","    stop_words=\"english\",\n","    preprocessor=clean_text\n",")\n","\n","# fit the vectorizer on the training text\n","vectorizer.fit(training_texts)\n","\n","# get the vectorizer's vocabulary\n","inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\n","vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]\n","\n","# vectorization example\n","pd.DataFrame(\n","    data=vectorizer.transform(test_texts).toarray(),\n","    index=[\"test sentence\"],\n","    columns=vocabulary\n",")\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import LinearSVC\n","\n","\n","# Transform each text into a vector of word counts\n","vectorizer = CountVectorizer(stop_words=\"english\",\n","                             preprocessor=clean_text)\n","\n","training_features = vectorizer.fit_transform(train_data[\"text\"])    \n","test_features = vectorizer.transform(test_data[\"text\"])\n","\n","# Training\n","model = LinearSVC()\n","model.fit(training_features, train_data[\"sentiment\"])\n","y_pred = model.predict(test_features)\n","\n","# Evaluation\n","acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n","\n","print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))\n","\n","train_data, test_data = load_train_test_imdb_data(\n","    data_dir=\"flip\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2611,"status":"aborted","timestamp":1617522569438,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"dJysYtCX2bx5"},"outputs":[],"source":["clean_text(\"<div>This is not a sentence.<\\div>\").split()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2608,"status":"aborted","timestamp":1617522569441,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"wMqIJEFOph92"},"outputs":[],"source":["batch_size = 32\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"flip.csv\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=1337,\n",")\n","raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"flip.csv\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=1337,\n",")\n","raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"flip.csv\", batch_size=batch_size\n",")\n","\n","print(\n","    \"Number of batches in raw_train_ds: %d\"\n","    % tf.data.experimental.cardinality(raw_train_ds)\n",")\n","print(\n","    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",")\n","print(\n","    \"Number of batches in raw_test_ds: %d\"\n","    % tf.data.experimental.cardinality(raw_test_ds)\n",")"]},{"cell_type":"markdown","metadata":{"id":"SM3QA_O4ph93"},"source":["Let's preview a few samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2599,"status":"aborted","timestamp":1617522569442,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"RN0ucc6vph93"},"outputs":[],"source":["# It's important to take a look at your raw data to ensure your normalization\n","# and tokenization will work as expected. We can do that by taking a few\n","# examples from the training set and looking at them.\n","# This is one of the places where eager execution shines:\n","# we can just evaluate these tensors using .numpy()\n","# instead of needing to evaluate them in a Session/Graph context.\n","for text_batch, label_batch in raw_train_ds.take(1):\n","    for i in range(5):\n","        print(text_batch.numpy()[i])\n","        print(label_batch.numpy()[i])"]},{"cell_type":"markdown","metadata":{"id":"Bc0jXwMAph93"},"source":["## Prepare the data\n","\n","In particular, we remove `<br />` tags."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2596,"status":"aborted","timestamp":1617522569442,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"Qf083tB8ph93"},"outputs":[],"source":["from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","import string\n","import re\n","\n","# Having looked at our data above, we see that the raw text contains HTML break\n","# tags of the form '<br />'. These tags will not be removed by the default\n","# standardizer (which doesn't strip HTML). Because of this, we will need to\n","# create a custom standardization function.\n","def custom_standardization(input_data):\n","    lowercase = tf.strings.lower(input_data)\n","    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n","    return tf.strings.regex_replace(\n","        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n","    )\n","\n","\n","# Model constants.\n","max_features = 20000\n","embedding_dim = 128\n","sequence_length = 500\n","\n","# Now that we have our custom standardization, we can instantiate our text\n","# vectorization layer. We are using this layer to normalize, split, and map\n","# strings to integers, so we set our 'output_mode' to 'int'.\n","# Note that we're using the default split function,\n","# and the custom standardization defined above.\n","# We also set an explicit maximum sequence length, since the CNNs later in our\n","# model won't support ragged sequences.\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=max_features,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","\n","# Now that the vocab layer has been created, call `adapt` on a text-only\n","# dataset to create the vocabulary. You don't have to batch, but for very large\n","# datasets this means you're not keeping spare copies of the dataset in memory.\n","\n","# Let's make a text-only dataset (no labels):\n","text_ds = raw_train_ds.map(lambda x, y: x)\n","# Let's call `adapt`:\n","vectorize_layer.adapt(text_ds)"]},{"cell_type":"markdown","metadata":{"id":"dzjdno51ph94"},"source":["## Two options to vectorize the data\n","\n","There are 2 ways we can use our text vectorization layer:\n","\n","**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n"," strings, like this:"]},{"cell_type":"markdown","metadata":{"id":"g2priRwiph94"},"source":["```python\n","text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n","x = vectorize_layer(text_input)\n","x = layers.Embedding(max_features + 1, embedding_dim)(x)\n","...\n","```\n","\n","**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n"," feed it into a model that expects integer sequences as inputs.\n","\n","An important difference between the two is that option 2 enables you to do\n","**asynchronous CPU processing and buffering** of your data when training on GPU.\n","So if you're training the model on GPU, you probably want to go with this option to get\n"," the best performance. This is what we will do below.\n","\n","If we were to export our model to production, we'd ship a model that accepts raw\n","strings as input, like in the code snippet for option 1 above. This can be done after\n"," training. We do this in the last section.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2595,"status":"aborted","timestamp":1617522569443,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"xYVLBXLoph95"},"outputs":[],"source":["\n","def vectorize_text(text, label):\n","    text = tf.expand_dims(text, -1)\n","    return vectorize_layer(text), label\n","\n","\n","# Vectorize the data.\n","train_ds = raw_train_ds.map(vectorize_text)\n","val_ds = raw_val_ds.map(vectorize_text)\n","test_ds = raw_test_ds.map(vectorize_text)\n","\n","# Do async prefetching / buffering of the data for best performance on GPU.\n","train_ds = train_ds.cache().prefetch(buffer_size=10)\n","val_ds = val_ds.cache().prefetch(buffer_size=10)\n","test_ds = test_ds.cache().prefetch(buffer_size=10)"]},{"cell_type":"markdown","metadata":{"id":"dHgvXDTiph95"},"source":["## Build a model\n","\n","We choose a simple 1D convnet starting with an `Embedding` layer."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2593,"status":"aborted","timestamp":1617522569443,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"i6Vb06ukph95"},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","# A integer input for vocab indices.\n","inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n","\n","# Next, we add a layer to map those vocab indices into a space of dimensionality\n","# 'embedding_dim'.\n","x = layers.Embedding(max_features, embedding_dim)(inputs)\n","x = layers.Dropout(0.5)(x)\n","\n","# Conv1D + global max pooling\n","x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n","x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","\n","# We add a vanilla hidden layer:\n","x = layers.Dense(128, activation=\"relu\")(x)\n","x = layers.Dropout(0.5)(x)\n","\n","# We project onto a single unit output layer, and squash it with a sigmoid:\n","predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n","\n","model = tf.keras.Model(inputs, predictions)\n","\n","# Compile the model with binary crossentropy loss and an adam optimizer.\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"6ZebIrG4ph96"},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2591,"status":"aborted","timestamp":1617522569443,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"Is_tzPTQph96"},"outputs":[],"source":["epochs = 3\n","\n","# Fit the model using the train and test datasets.\n","model.fit(train_ds, validation_data=val_ds, epochs=epochs)"]},{"cell_type":"markdown","metadata":{"id":"jKsrlWHaph96"},"source":["## Evaluate the model on the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2590,"status":"aborted","timestamp":1617522569444,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"MPg8AdOgph96"},"outputs":[],"source":["model.evaluate(test_ds)"]},{"cell_type":"markdown","metadata":{"id":"QmvBT3Qtph97"},"source":["## Make an end-to-end model\n","\n","If you want to obtain a model capable of processing raw strings, you can simply\n","create a new model (using the weights we just trained):"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2588,"status":"aborted","timestamp":1617522569444,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"DBVkaY1yph97"},"outputs":[],"source":["# A string input\n","inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n","# Turn strings into vocab indices\n","indices = vectorize_layer(inputs)\n","# Turn vocab indices into predictions\n","outputs = model(indices)\n","\n","# Our end to end model\n","end_to_end_model = tf.keras.Model(inputs, outputs)\n","end_to_end_model.compile(\n","    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",")\n","\n","# Test it with `raw_test_ds`, which yields raw strings\n","end_to_end_model.evaluate(raw_test_ds)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":1646,"status":"ok","timestamp":1617523109198,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"g5xaWsiL49u-"},"outputs":[],"source":["import numpy as np\n","from numpy import random\n","import re\n","import pickle\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","\n","from pathlib import Path\n","from gensim.models import Word2Vec\n","from gensim.models.keyedvectors import KeyedVectors\n","\n","%matplotlib inline\n","matplotlib.style.use('ggplot')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":1037,"status":"error","timestamp":1617523149423,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"Ode3CvpM4-7F","outputId":"3cfa0733-e235-4f78-9a13-599d8b68aec3"},"outputs":[],"source":["from dataset import Dataset\n","\n","from keras import losses, models, optimizers, initializers, regularizers\n","\n","from keras.models import Sequential, Model, model_from_json\n","\n","from keras.preprocessing.text import Tokenizer\n","\n","from keras.layers import (Layer, Input, Flatten, Dropout, BatchNormalization, Reshape, Embedding,\n","                          MaxPool1D, AveragePooling1D, GlobalAveragePooling1D,\n","                          Conv1D, SeparableConv1D, Dense, LeakyReLU, ReLU, Activation,\n","                          LSTM, SimpleRNNCell, Bidirectional)"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":999,"status":"ok","timestamp":1617523446298,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"hx_3Y9IU6JMs"},"outputs":[],"source":["dataset.columns = dataset.columns.str.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":1199,"status":"error","timestamp":1617523451412,"user":{"displayName":"Noor Fatima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ4Zfr2Vn4peY_2k_2oI_g9wH3B_Oxb31fpb99SA=s64","userId":"06775967229168824100"},"user_tz":-330},"id":"qKySQ5_r5IuY","outputId":"02608413-bda7-4cc5-9f2a-2c63c14b6587"},"outputs":[],"source":["text_field='name'\n","\n","#dataset = Dataset(filename)\n","dataset = pd.read_csv(\"/content/flip.csv\")\n","dataset.load(text_field=text_field, label_field='category', root_label=True)\n","dataset.filer_data()\n","dataset.text_info()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"testt","provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/text_classification_from_scratch.ipynb","timestamp":1617520264912}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noor1\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "from dataset import Dataset\n",
    "\n",
    "from keras import losses, models, optimizers, initializers, regularizers\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.layers import (Layer, Input, Flatten, Dropout, BatchNormalization, Reshape, Embedding,\n",
    "                          MaxPool1D, AveragePooling1D, GlobalAveragePooling1D,\n",
    "                          Conv1D, SeparableConv1D, Dense, LeakyReLU, ReLU, Activation,\n",
    "                          LSTM, SimpleRNNCell, Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"flip.csv\", names={'product_name','product_category','description'})\n",
    "#filename = 'datasets/amazon_co-ecommerce_sample.csv'\n",
    "#text_field='product_name'\n",
    "\n",
    "#dataset = Dataset(filename)\n",
    "#data=pd.read_pickle(text_field=text_field, label_field='product_category', root_label=True)\n",
    "#dataset.filer_data()\n",
    "#dataset.text_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'flip.csv'\n",
    "text_field='product_name'\n",
    "\n",
    "#dataset = pd.read_csv(\"flip.csv\", names={'product_name','product_category','description'})\n",
    "dataset = Dataset(filename)\n",
    "dataset.load(text_field=text_field, label_field='product_category', root_label=True)\n",
    "dataset.filer_data()\n",
    "dataset.text_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = dataset.tokenize()\n",
    "label_encoder = dataset.label_encode()\n",
    "\n",
    "data_train, data_test = train_test_split(dataset.data, test_size=0.15)\n",
    "    \n",
    "token_train = tokenizer.texts_to_sequences(data_train.text)\n",
    "text_train = pad_sequences(token_train, maxlen=dataset.max_text())\n",
    "label_train = label_encoder.transform(data_train.label)\n",
    "        \n",
    "token_test = tokenizer.texts_to_sequences(data_test.text)\n",
    "text_test = pad_sequences(token_test, maxlen=dataset.max_text())\n",
    "label_test = label_encoder.transform(data_test.label)\n",
    "\n",
    "#dataset=dataset.apply(word_tokenize)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word2vec = True\n",
    "if train_word2vec:\n",
    "    size = 500\n",
    "    max_vocab_size = 20000\n",
    "\n",
    "    sentences = dataset.text.transform(lambda t: t.split())\n",
    "\n",
    "    embedding = Word2Vec(sentences, size=size,\n",
    "                         max_vocab_size=max_vocab_size)\n",
    "\n",
    "    output_file = 'embeddings/{}_s{}.txt'.format('word2vec', size)\n",
    "    embedding.wv.save_word2vec_format(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_filename = 'word2vec.ipynb'\n",
    "\n",
    "embedding = KeyedVectors.load_word2vec_format(embedding_filename)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding.vector_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = embeddings.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                            output_dim=embedding.vector_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=dataset.max_text(),\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_sepcnn = True\n",
    "if use_sepcnn: \n",
    "    \n",
    "    # Hyper Parameters\n",
    "    blocks       = 1\n",
    "    filters      = 32\n",
    "    kernel_size  = 3\n",
    "    dropout_rate = 0.2\n",
    "    pool_size    = 3\n",
    "    \n",
    "    #input_layer = Input(shape=(dataset.max_text(),), dtype='int32')\n",
    "    input_layer = tf.keras.Input(shape=(32,),dtype='int32')\n",
    "    x = embedding_layer(input_layer)\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        x = Dropout(rate=dropout_rate)(x)   \n",
    "        x = SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu',bias_initializer='random_uniform', depthwise_initializer='random_uniform',padding='same')(x)\n",
    "        x = SeparableConv1D(filters=filters, kernel_size=kernel_size, activation='relu',bias_initializer='random_uniform', depthwise_initializer='random_uniform',padding='same')(x) \n",
    "        x = MaxPool1D(pool_size=pool_size)(x) \n",
    "        x = SeparableConv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu',bias_initializer='random_uniform', depthwise_initializer='random_uniform',padding='same')(x) \n",
    "        x = SeparableConv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu',bias_initializer='random_uniform', depthwise_initializer='random_uniform',padding='same')(x) \n",
    "        x = GlobalAveragePooling1D()(x) \n",
    "        x = Dropout(rate=dropout_rate)(x) \n",
    "        output_layer = Dense(dataset.label_unique.size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.name = 'sepcnn_' + str(embedding.vector_size) + '_' + text_field\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jatana_lstm = True\n",
    "if use_jatana_lstm:\n",
    "    \n",
    "    # Hyper Parameters\n",
    "    units       = 100\n",
    "    drop_ratio  = 0.3\n",
    "    drop_rec_ratio = 0.3\n",
    "    \n",
    "    input_layer = Input(shape=(dataset.max_text(),), dtype='int32')\n",
    "    x = embedding_layer(input_layer)\n",
    "    x = Bidirectional(LSTM(units, dropout=drop_ratio, recurrent_dropout=drop_rec_ratio))(x)\n",
    "    output_layer = Dense(dataset.label_unique.size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.name = 'lstm_jatana_' + str(embedding.vector_size) + '_' + text_field\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('created_models/'+ model.name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODELS = 'created_models/'\n",
    "MODEL = PATH_MODELS + 'sepcnn_100_name.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
